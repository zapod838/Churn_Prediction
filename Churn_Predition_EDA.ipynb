{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67439e3b-34bd-4104-90dc-e5524f91ceed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#importing trai.csv\n",
    "train_df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "612dba41-e6b4-4725-859b-6a228587c2d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Data Preprocessing:** *Encoding Categorical variables*\n",
    "> *Machine learning models prefer numerical data. Categorical data, often in text format, needs conversion to numerical form for compatibility. This allows the model to understand and process the data for effective learning.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6003bb52-3568-4a12-b939-b09015a961ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, Normalizer\n",
    "import pandas as pd\n",
    "\n",
    "# List of categorical columns to be one-hot encoded\n",
    "categorical_columns = [\n",
    "    'PaymentMethod', 'PaperlessBilling',\n",
    "    'ContentType', 'MultiDeviceAccess', 'DeviceRegistered',\n",
    "    'GenrePreference', 'Gender', 'ParentalControl', 'SubtitlesEnabled'\n",
    "]\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_categorical = onehot_encoder.fit_transform(train_df[categorical_columns])\n",
    "\n",
    "# Convert the categorical encoded data into a DataFrame with column names\n",
    "encoded_categorical_df = pd.DataFrame(\n",
    "    encoded_categorical,\n",
    "    columns=onehot_encoder.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa85ade-42a0-422e-b8f5-7649891203a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the order of the categories\n",
    "subscription_type_ordering = ['Basic', 'Standard', 'Premium']\n",
    "\n",
    "# Create the OrdinalEncoder object with the specified ordering\n",
    "ordinal_encoder = OrdinalEncoder(categories=[subscription_type_ordering])\n",
    "\n",
    "# Fit and transform the data\n",
    "train_df['SubscriptionTypeEncoded'] = ordinal_encoder.fit_transform(train_df[['SubscriptionType']])\n",
    "\n",
    "# Drop the original categorical columns and concatenate the one-hot encoded columns\n",
    "train_df_new = train_df.drop(categorical_columns + ['SubscriptionType'], axis=1)\n",
    "train_df_encoded = pd.concat([train_df_new, encoded_categorical_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74e44b6-091b-4220-803e-a4da0866f762",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Numerical Scaling (Data Transformation):** Since the data does not follow  Gaussian distribution. \n",
    ">  *Feature scaling in machine learning prevents features with larger values from dominating the model. This improves model training by creating a smoother learning process and reducing the risk of overfitting.*# List of numerical columns to be normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b8a964-9f43-4619-8760-e639d8891a2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of numerical columns to be normalized\n",
    "numerical_columns = [\n",
    "    'AccountAge', 'MonthlyCharges', 'TotalCharges',\n",
    "    'ViewingHoursPerWeek', 'AverageViewingDuration',\n",
    "    'ContentDownloadsPerMonth', 'UserRating', 'SupportTicketsPerMonth',\n",
    "    'WatchlistSize'\n",
    "]\n",
    "\n",
    "# Create the Normalizer object\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Fit and transform the numerical columns\n",
    "train_df_encoded[numerical_columns] = normalizer.fit_transform(train_df_encoded[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4f1901-1c0e-48fc-961a-b202d203c2c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Churn\n",
       "0    199605\n",
       "1     44182\n",
       "Name: AccountAge, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cheching the distribution of predictive variables features\n",
    "train_df.groupby('Churn')[\"AccountAge\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb84a5e-bfe2-407e-8876-576bf7dc35a5",
     "showTitle": true,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dropping CustomerID column as it has no relevance with predictive variable\n",
    "transformed_df = train_df_encoded.drop(['CustomerID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef4ebca-6706-4027-a1dd-a8eb10f46420",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating X and y as features and target variables\n",
    "X = transformed_df.drop(['Churn'], axis = 1)\n",
    "Y = transformed_df.pop('Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26a55e3-e949-45d0-b9f2-3fdc6748c53d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churn\n",
      "0    199605\n",
      "1    192261\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Create the ADASYN object\n",
    "ada = ADASYN(random_state=42)\n",
    "\n",
    "# Fit and resample the data\n",
    "X_resampled, y_resampled = ada.fit_resample(X, Y)\n",
    "\n",
    "# Counting the values for each class after resampling\n",
    "class_counts = y_resampled.value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8dc0d7d-8553-4066-83b9-5928ab48a5dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame from the resampled data\n",
    "balanced_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "balanced_df['Churn'] = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.to_csv(\"A:/DA_DS_BA/Projects/Churn Prediction Challenge/Churn_Streamlit/balanced_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Churn_Predition_EDA",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
