{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0acb895-c5d8-4228-9a72-ac9a6c039ac4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlflow import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaac9989-5987-4293-afb7-150dce9680f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391866, 37)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the balancced_df data from the Parquet file\n",
    "transformed_df = pd.read_csv(\"balanced_df.csv\")\n",
    "transformed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f538fe-41ac-486a-86c1-8d3c6760652e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = transformed_df.corr()\n",
    "\n",
    "# Sort the correlations by the 'Churn' column, and remove the self-correlation\n",
    "sorted_correlation = correlation_matrix['Churn'].drop('Churn', errors='ignore').sort_values(key=np.abs, ascending=False)\n",
    "\n",
    "# Step 1: Select the subset of features\n",
    "features = [\n",
    "    'MonthlyCharges', 'SupportTicketsPerMonth', 'UserRating', \n",
    "    'WatchlistSize', 'AccountAge', 'TotalCharges', \n",
    "    'ViewingHoursPerWeek', 'ContentDownloadsPerMonth', \n",
    "    'AverageViewingDuration'\n",
    "]\n",
    "\n",
    "X_subset = transformed_df[features]\n",
    "\n",
    "# Step 2: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_subset)\n",
    "\n",
    "# Step 3: Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c599958c-03a8-4409-9ba4-29d51d8edf60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = transformed_df.drop(['Churn'], axis=1)\n",
    "y = transformed_df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca86f43b-073b-4c92-9f1a-010a2bf6ffad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify the most correlated features for PCA\n",
    "most_correlated_features = ['MonthlyCharges', 'SupportTicketsPerMonth', 'UserRating', \n",
    "                            'WatchlistSize', 'AccountAge', 'TotalCharges', \n",
    "                            'ViewingHoursPerWeek', 'ContentDownloadsPerMonth', \n",
    "                            'AverageViewingDuration']\n",
    "\n",
    "# Define the feature processing for PCA\n",
    "pca_pipeline = Pipeline(steps=[('scale', StandardScaler()), ('pca', PCA(n_components=9))])\n",
    "\n",
    "# Pipeline for already encoded features\n",
    "# Using FunctionTransformer to create a no-op (no operation) pipeline step\n",
    "encoded_features = [\n",
    "    'SubscriptionTypeEncoded','PaymentMethod_Bank transfer','PaymentMethod_Credit card',\n",
    "    'PaymentMethod_Electronic check','PaymentMethod_Mailed check','PaperlessBilling_No',\n",
    "    'PaperlessBilling_Yes','ContentType_Both','ContentType_Movies',\n",
    "    'ContentType_TV Shows','MultiDeviceAccess_No','MultiDeviceAccess_Yes',\n",
    "    'DeviceRegistered_Computer','DeviceRegistered_Mobile','DeviceRegistered_TV',\n",
    "    'DeviceRegistered_Tablet','GenrePreference_Action','GenrePreference_Comedy',\n",
    "    'GenrePreference_Drama','GenrePreference_Fantasy','GenrePreference_Sci-Fi',\n",
    "    'Gender_Female','Gender_Male','ParentalControl_No',\n",
    "    'ParentalControl_Yes','SubtitlesEnabled_No','SubtitlesEnabled_Yes'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308eaff3-4acb-4036-814b-a8ed4a8afb62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pass_through_pipeline = Pipeline([('identity', FunctionTransformer())])\n",
    "\n",
    "# Define the ColumnTransformer for selecting appropriate features\n",
    "preprocess_pipeline = ColumnTransformer(transformers=[('pca_features', pca_pipeline, most_correlated_features),\n",
    "                                                      ('encoded_features', pass_through_pipeline, encoded_features)\n",
    "                                                      ], remainder='drop')  # 'drop' drops features not specified in transformers\n",
    "\n",
    "# Create a pipeline that combines feature processing, resampling, and classifier\n",
    "pipeline = ImbPipeline(steps=[('preprocess', preprocess_pipeline), \n",
    "                              ('classifier', RandomForestClassifier(random_state=42,class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1592a114-2e11-4c21-912e-43d5cab190cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    #Fit the model (ensure your data is ready and split as needed)\n",
    "    pipeline.fit(X, y)\n",
    "    \n",
    "    #Log the model\n",
    "    mlflow.sklearn.log_model(pipeline, \"churn_prediction_pipeline\")\n",
    "\n",
    "    #Log additional information, if necessary\n",
    "    mlflow.log_param(\"features_used\", features)\n",
    "    mlflow.log_param(\"PCA_components\", 9)\n",
    "    mlflow.log_param(\"classifier\", \"RandomForest\")\n",
    "# The run ends when exiting the 'with' block"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Churn_Final_Model",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
